{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/moellava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-16 12:08:52,226] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/moellava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [08:27<00:00, 253.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-16 12:17:25,163] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,167] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,170] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,172] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,175] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,178] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,180] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,183] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,186] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,188] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,192] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n",
      "[2024-05-16 12:17:25,195] [INFO] [logging.py:96:log_dist] [Rank -1] Creating MoE layer with num_experts: 4 | num_local_experts: 4 | expert_parallel_size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-16 12:17:29,151] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2024-05-16 12:17:29,152] [INFO] [comm.py:594:init_distributed] cdb=None\n",
      "[2024-05-16 12:17:29,152] [INFO] [comm.py:609:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-16 12:17:29,173] [INFO] [comm.py:659:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.23.81.17, master_port=29500\n",
      "[2024-05-16 12:17:29,174] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-05-16 12:17:29,176] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-16 12:17:29,177] [INFO] [logging.py:96:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/moellava/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from moellava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from moellava.conversation import conv_templates, SeparatorStyle\n",
    "from moellava.model.builder import load_pretrained_model\n",
    "from moellava.utils import disable_torch_init\n",
    "from moellava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n",
    "\n",
    "\n",
    "disable_torch_init()\n",
    "\n",
    "model_path = 'LanguageBind/MoE-LLaVA-StableLM-1.6B-4e'  # choose a model\n",
    "device = 'cuda'\n",
    "load_4bit, load_8bit = False, False\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, processor, context_len = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation(system=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", roles=('USER', 'ASSISTANT'), messages=[], offset=0, sep_style=<SeparatorStyle.TWO: 2>, sep=' ', sep2='<|endoftext|>', version='phi', skip_next=False)\n",
      "<class 'moellava.conversation.Conversation'>\n",
      "('USER', 'ASSISTANT')\n",
      "当前已分配显存量：6406559232 字节\n",
      "ASSISTANT: Describe the image.\n",
      "[['USER', '<image>\\nDescribe the image.'], ['ASSISTANT', None]]\n",
      "<class 'moellava.mm_utils.KeywordsStoppingCriteria'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_processor = processor['image']\n",
    "conv_mode = \"phi\"  # phi or qwen or stablelm\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "print(conv)\n",
    "print(type(conv))\n",
    "roles = conv.roles\n",
    "print(roles)\n",
    "\n",
    "# 获取当前已分配的显存量\n",
    "allocated_memory = 0\n",
    "print(f\"当前已分配显存量：{torch.cuda.memory_allocated()-allocated_memory} 字节\")\n",
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "\n",
    "\n",
    "root_path = 'moellava/serve/examples'\n",
    "img_pth = os.listdir(root_path)\n",
    "img_list = [os.path.join(root_path,img) for img in img_pth]\n",
    "\n",
    "image = 'moellava/serve/examples/WechatIMG4.jpg'\n",
    "inp = 'Describe the image.'\n",
    "# inp_list = [inp for i in range(len(img_list))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"{roles[1]}: {inp}\")\n",
    "inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n",
    "\n",
    "conv.append_message(conv.roles[0], inp)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "print(conv.messages)\n",
    "prompt = conv.get_prompt()\n",
    "\n",
    "input_id = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "# input_id_list = [input_id for _ in range(len(img_list))]\n",
    "# input_ids = torch.cat(input_id_list,0)\n",
    "\n",
    "# print(input_ids.shape)\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_id)\n",
    "print(type(stopping_criteria))\n",
    "\n",
    "\n",
    "\n",
    "# print(f\"当前已分配显存量：{torch.cuda.memory_allocated()-allocated_memory} 字节\")\n",
    "# allocated_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference(image):\n",
    "    image_tensor = image_processor.preprocess(Image.open(image).convert('RGB'), return_tensors='pt')['pixel_values'].to(model.device, dtype=torch.float16)\n",
    "    # img_tensor_list = [image_processor.preprocess(Image.open(image).convert('RGB'), return_tensors='pt')['pixel_values'].to(model.device, dtype=torch.float16) for image in img_list]\n",
    "    # img_tensors = torch.cat(img_tensor_list)\n",
    "    # print(img_tensors.shape)\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_id,\n",
    "            images=image_tensor,\n",
    "            do_sample=True,\n",
    "            temperature=0.2,\n",
    "            max_new_tokens=1024,\n",
    "            use_cache=True,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "            pad_token_id = 100257,\n",
    "            eos_token_id = 100257\n",
    "            )\n",
    "\n",
    "    return output_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "The image depicts a beautiful beach scene with a city in the background. The city is lit up at night, creating a stunning contrast between the vibrant lights and the serene ocean. The beach is filled with palm trees, and the water appears to be calm and inviting. The overall atmosphere of the scene is picturesque and inviting, showcasing the beauty of both the city and the beach.\n",
      "The image features a black and white cat sitting on a person's arm. The cat appears to be looking at the camera, and its eyes are wide open. The scene takes place indoors, with the person's arm visible in the foreground. The cat seems to be enjoying the attention and affection from its owner.\n",
      "The image shows a man standing on the back of a yellow taxi cab, using a clothes iron to press clothes. The scene takes place on a busy city street, with other vehicles such as a car and a bus visible in the background. The man is wearing a yellow shirt and appears to be focused on his task.\n",
      "The image features a bed with a quilt, and a TV screen is mounted on the wall above the bed. The screen displays a picture of a frying pan, which is likely a movie or a video playing on the TV. The bed is covered with a checkered blanket, and the TV is positioned above the bed, making it a cozy and comfortable setup for watching movies or videos.\n",
      "The image features a large elephant standing in shallow water, with its feet submerged. The elephant's legs are positioned in a way that it appears to be wearing jeans or pants. The scene is set in a beach environment, with the elephant standing near the shore.\n",
      "The image features a wooden pier or dock extending out over a body of water, with a mountain range in the background. The pier is located near a lake, and there is a bench on the pier. The scene is serene and picturesque, with the wooden pier and the mountain range creating a beautiful and tranquil atmosphere.\n",
      "每张照片花费平均时间为3.2280099789301553s\n",
      "当前已分配显存量：0 字节\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "for image in img_list:\n",
    "\n",
    "    output_ids = inference(image)\n",
    "    outputs = tokenizer.decode(output_ids[0, input_id.shape[1]:], skip_special_tokens=True).strip()\n",
    "    print(outputs)\n",
    "# print(input_id.shape)\n",
    "# print(output_ids.shape)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "sum_time = end_time - start_time\n",
    "avg_time = sum_time/len(img_list)\n",
    "print(f\"每张照片花费平均时间为{avg_time}s\")\n",
    "print(f\"当前已分配显存量：{torch.cuda.memory_allocated()-allocated_memory} 字节\")\n",
    "allocated_memory = torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'caption': 'a city at sunset with a large body of water'}, 'uid': '1005_182821815599579138_182823065434980352_5e664d90-1069-4c69-9470-02d481f61bf9'}\n",
      "{'params': {'caption': \"a black and white cat laying on a person's lap\"}, 'uid': '1005_182821815599579138_182823065434980352_5e664d90-1069-4c69-9470-02d481f61bf9'}\n",
      "{'params': {'caption': 'a woman riding a scooter down a city street'}, 'uid': '1005_182821815599579138_182823065434980352_5e664d90-1069-4c69-9470-02d481f61bf9'}\n",
      "{'params': {'caption': 'a television screen with a clock on top of it'}, 'uid': '1005_182821815599579138_182823065434980352_5e664d90-1069-4c69-9470-02d481f61bf9'}\n",
      "{'params': {'caption': 'two elephants standing next to each other on a beach'}, 'uid': '1005_182821815599579138_182823065434980352_5e664d90-1069-4c69-9470-02d481f61bf9'}\n",
      "{'params': {'caption': 'a wooden dock on a body of water'}, 'uid': '1005_182821815599579138_182823065434980352_5e664d90-1069-4c69-9470-02d481f61bf9'}\n",
      "1.5796031951904297\n",
      "当前已分配显存量：0 字节\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "def get_dispatch_req(dev):\n",
    "    if dev:\n",
    "        url = 'http://test-infra-internal.conss.co/api/ms/v1/infra-internal/aigc/model-dispatch'\n",
    "\n",
    "    else:\n",
    "        url = 'http://infra-internal-us.conss.co/api/ms/v1/infra-internal/aigc/model-dispatch'\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def req_caption(img_base64, url):\n",
    "\n",
    "    gen_params = {\n",
    "                \"uid\": \"1005_182821815599579138_182823065434980352_5e664d90-1069-4c69-9470-02d481f61bf9\",\n",
    "                \"dispatch_model_type\": 6,\n",
    "                \"dispatch_proxy_mode\": 2,\n",
    "                \"params\": {\n",
    "                    \"img\": ''\n",
    "                }\n",
    "    }\n",
    "\n",
    "    error_msg = None\n",
    "\n",
    "    try:\n",
    "        gen_params['params']['img'] = img_base64\n",
    "        res = requests.post(url,\n",
    "        json=gen_params,\n",
    "        stream=True,\n",
    "        timeout=10)  # Send the HTTP request with a timeout of 10 seconds\n",
    "        res.raise_for_status()  # Raise an exception if the response status code is not 2xx\n",
    "\n",
    "        return res.json(), error_msg\n",
    "\n",
    "    except requests.exceptions.Timeout as e:\n",
    "        return None, str(e)\n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        # Handle other request errors (e.g., service does not exist)\n",
    "\n",
    "        return None, str(requests.exceptions.RequestException)\n",
    "\n",
    "def get_img_caption(image_path):\n",
    "        dev = True\n",
    "        url = get_dispatch_req(dev)\n",
    "        # image_path = '/mnt/MoE-LLaVA/moellava/serve/examples/extreme_ironing.jpg'\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "\n",
    "        print(req_caption(img_base64, url)[0])\n",
    "\n",
    "        \n",
    "start_time = time.time()\n",
    "for img in img_list:\n",
    "    \n",
    "    get_img_caption(img)\n",
    "end_time = time.time()\n",
    "avg_time = -(start_time - end_time)/len(img_list)\n",
    "print(avg_time)\n",
    "print(f\"当前已分配显存量：{torch.cuda.memory_allocated()-allocated_memory} 字节\")\n",
    "allocated_memory = torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/anaconda3/envs/moellava/lib/python310.zip', '/root/anaconda3/envs/moellava/lib/python3.10', '/root/anaconda3/envs/moellava/lib/python3.10/lib-dynload', '', '/root/anaconda3/envs/moellava/lib/python3.10/site-packages', '__editable__.moellava-1.0.0.finder.__path_hook__', '/tmp/tmp8hpushlx']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moellava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
